---
title: Week 8 Forecast
author: Cathy Stanton
date: '2024-10-28'
slug: week-8-forecast
categories: []
tags: []
---

<h1>Week 8: October 28, 2024</h1>  
This blog is an ongoing project for Professor Ryan Enos' Election Analytics Course at Harvard College (GOV 1347, Fall 2024). It will be updated weekly with posts analyzing how different features impact the likelihood of Kamala Harris (D) or Donald Trump (R) winning the 2024 U.S. Presidential Election or winning specific states in the election. The blog will culminate in a final predictive model for the outcome of the general election.

<h2>Context & Question</h2>
In this penultimate post, I will gear up to make a final state-by-state election prediction next Monday, November 4th. On this blog, I've looked at different predictors of candidate success (incumbency, ad tone, the value of the US dollar). I've also built a Bayesian Model to predict candidate success based on polling averages and updated by early voting returns. My final predictive model will be Bayesian and updated by early voting returns. To develop a prior probability, I plan to run a principal component analysis for each state. This will tell me what combination of features or variables the winner of each state tends to vary along--it's useful in a context with so many predictors and will allow me to incorporate more of the variables we've studied over the course of the semester. Since early voting is ongoing, I won't yet bring in early voting returns to this model; I'll incorporate them on Sunday, November 2nd.

<h2>The Data</h2>
I've loaded datasets about polling averages, various economic conditions (GDP growth rates, nationwide unemployment rate, the consumer price index, and real disposable personal income), turnout rates, demographics, and ad spending. I tried to use as many state-specific variables as possible but some of these are constant across the nation. They're still important to include however because they may have greater *effects* in some states than others, or, in combination with other features, may predict the outcome of a state. To predict a binary winner from each state (Democrat or Republican) I will apply a sigmoid function to the line that explains variance of 2-party vote share along the first **two** principle components. It's worth noting the different amounts of data available for different predictors (e.g. some datasets have data going back to 1948, others to 1968, others to 2000, etc.). I will group data by state and election cycle and look at which principal components are best at predicting winners between elections. One challenge is that the principal components that best predict candidate success may change substantially over time, based on the priorities of Americans and the contexts in which they're living. Some of the data in this study goes back to 1968 and even 1948--it wouldn't be unreasonable to assume that the factors influencing voters today are different than those influencing voters back then.  

```{r, echo=F, include=F}
library(tidyverse)
library(lubridate)

ec_framework <- read.csv("corrected_ec_1948_2024.csv")
state_results <- read.csv("state_popvote_1948_2020.csv") %>%
  select(year, state, D_pv, R_pv, D_pv2p, R_pv2p, votes_D, votes_R, total_votes, two_party_votes)
state_polls <- read.csv("state_polls_1968-2024.csv") 

ec_results <- ec_framework %>% left_join(state_results, by=join_by(state, year))

unemployment <- read.csv("fred_unemployment_national_1948_2024.csv")
cpi_data <- read.csv("fred_CPI_urban_monthly_1947_2024.csv")
rdpi_data <- read.csv("fred_rdpi_national_1959_2024.csv")
gdp_growth <- read.csv("fred_gdp_growth_quarterly_national_1947_2024.csv")

# clean GDP Dates for joining
gdp_growth$DATE <- as.Date(gdp_growth$DATE, format="%m/%d/%Y") %m+% years(1900)
for (i in 1:nrow(gdp_growth)) {
  if (gdp_growth$DATE[i] <= as.Date("1925-01-01", format="%Y-%m-%d")) {
    gdp_growth$DATE[i] = gdp_growth$DATE[i] %m+% years(100)
  }
}

cpi_data$DATE <- as.Date(cpi_data$DATE, format="%Y-%m-%d")
rdpi_data$DATE <- as.Date(rdpi_data$DATE, format="%Y-%m-%d")
unemployment$DATE <- as.Date(unemployment$DATE, format="%Y-%m-%d")

econ_data <- cpi_data %>%
              left_join(gdp_growth, join_by(DATE)) %>%
              left_join(unemployment, join_by(DATE)) %>%
              left_join(rdpi_data, join_by(DATE))
colnames(econ_data) <- c("DATE", "cpi", "gdp_growth", "unemployment_rate", "rdpi")

# load turnout data
turnout <- read.csv("state_turnout_1980_2022.csv")

# load demographic data
demographics <- read.csv("demographics.csv")

# join with states
ec_results <- ec_results %>% left_join(turnout, by = join_by(state, year)) %>% left_join(demographics, by = join_by(state, year))

# load ads data
# ads_spending <- read.csv("ad_campaigns_2000-2012.csv") %>%
                  # select(state, total_cost, cycle)

# ec_results <- ec_results %>% left_join(ads_spending, by = join_by(state, year==cycle))
```


<h2>Results</h2>
The first step in a principle component analysis is to normalize each column of data by subtracting its mean and dividing by its standard deviation. This ensures that, since we are trying to see which predictors are most salient and amplify them in our prediction, the disparate scales do not make a difference. If we had data that where a "small" difference could be considered a value of 1,000 (like in ads spending for example), and DATA where a "large" difference could be considered 1,000 (like RDPI), then we'd unintentionally treat the small changes in ads spending the same as large changes in RDPI. 

```{r, warning=F, echo=F}
library(corrr)
library(purrr)
# library(ggcorrrplot)

# clean up non numerical columns 
ec_results$vep_turnout <- as.numeric(strsplit(ec_results$vep_turnout, "%"))
ec_results$vep_highest_office <- as.numeric(strsplit(ec_results$vep_highest_office, "%"))
ec_results$vap_highest_office <- as.numeric(strsplit(ec_results$vap_highest_office, "%"))
ec_results$prison <- as.numeric(str_replace_all(ec_results$prison, ",", ""))
ec_results$probation <- as.numeric(str_replace_all(ec_results$probation, ",", ""))
ec_results$parole <- as.numeric(str_replace_all(ec_results$parole, ",", ""))
ec_results$total_ineligible <- as.numeric(str_replace_all(ec_results$total_ineligible, ",", ""))
ec_results$overseas_eligible <- as.numeric(str_replace_all(ec_results$overseas_eligible, ",", ""))
ec_results$noncitizen <- as.numeric(strsplit(ec_results$noncitizen, "%"))
ec_results$total_ballots <- as.numeric(str_replace_all(ec_results$total_ballots, ",", ""))
ec_results$highest_office_ballots <- as.numeric(str_replace_all(ec_results$highest_office_ballots, ",", ""))
ec_results$vep <- as.numeric(str_replace_all(ec_results$vep, ",", ""))
ec_results$vap <- as.numeric(str_replace_all(ec_results$vap, ",", ""))

# impute column mean for NA values, with help from AI
for (col in colnames(ec_results)) {
  ec_results[[col]][is.na(ec_results[[col]])] <- mean(ec_results[[col]], na.rm = TRUE)
}

ec_results_scaled <- scale(ec_results[,5:67])


ec_results_scaled <- cbind(ec_results[,1:3], ec_results_scaled)

# run a pca on the entire data
whole_pca <- princomp(ec_results_scaled[4:66])
summary(whole_pca)


# apply pca to each state and cycle 
#pca_results <- ec_results_scaled %>%
#                group_by(state, year) %>%
#                nest() %>%
#                select(-stateab) %>%
#                princomp()

```

The principal component analysis of all states and races in the dataset indicates that 90% of the variance in 2-party vote share can be explained by the first 13 (out of 63) principal components. That goes a LONG way in reducing the dimensionality of the dataset, and still allows us to factor in economic data, turnout trends, demographic data, and polling averages into our prediction. To explain 99% of the variance, we only need to use 30 principal components. In the final prediction, I seek to repreat this PCA on a state-by-state basis, predict a probability that the Democratic Candidate will win, treat it as a prior, and then update it based on the latest early voting returns.

<h2> Bonus Content!</h2>
Since the PCA will be expanded on in my final election prediction, I wanted to provide a few relevant visuals and predictions to help us better understand the state of the 2024 Election. It's been a few weeks since I made predictions based on polling, and I wanted to practice pulling data from Fivethirtyeight, so here are some visuals and explanations about the current polling and approval ratings for candidates in the U.S. Presidential Election. The first is approval rating for Congress, the Supreme Court, the President, and the Vice President over the past 4 years. As we can see, Congress had a markedly lower approval rating than the other 3 institutions/politicians, which I wasn't expecting. This is surprising because congressional elections are held more frequently than presidential elections so I would think that the people expressing their approval/disapproval would have a say over congress members more often. The approval rating of the Supreme Court fluctuated most steeply, but if you look closely, you can see it fluctuate *with* the approval rating of Congress, rising and falling together. This may indicate something about how everyday Americans answering approval polls conceptualize the Executive Branch of government versus the Judicial and Legislative.

```{r, echo=F, warnng=F}
library(maps)

vp_approval <- read.csv("vp_approval_polls.csv")
approval <- read.csv("approval_averages.csv")

ggplot(data=subset(approval, answer=="Approve"), mapping=aes(x=date, y=pct_estimate, group=politician.institution, color=politician.institution)) +
  geom_smooth() +
  scale_x_discrete(breaks=c("2021-01-01", "2024-10-27"))

```

Unsurprisingly, the approval ratings for Joe Biden and Kamala Harris varied together. This makes sense because they make up the leadership of the same administration (or at least they are the most visible figures). However, we see the approval rating for Kamala Harris spike towards the end of the 4-year term... around when she began her presidential campaign. This is interesting because this poll is based on her work in office, not her campaign. So it suggests that, by running for the presidency, she developed a more favorable impression about her work in the Biden Administration, among everyday Americans. This could be the result of her having a greater platform to talk about the work, her gaining more visibility, or Americans becoming optimistic about her chance at the presidency.

```{r}

```

